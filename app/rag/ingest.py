"""ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª â€” ØªÙ‚Ø·ÙŠØ¹ ÙˆØªØ®Ø²ÙŠÙ† ÙÙŠ ChromaDB"""

import os
import sys
import logging
from pathlib import Path
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø¬Ø°Ø±ÙŠ
sys.path.insert(0, str(Path(__file__).resolve().parent.parent.parent))

from app.config import get_settings

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")
logger = logging.getLogger(__name__)

DOCUMENTS_DIR = Path(__file__).resolve().parent.parent.parent / "documents"


def load_text_files() -> list[Document]:
    """ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª .txt Ù…Ù† Ù…Ø¬Ù„Ø¯ documents/"""
    documents = []

    if not DOCUMENTS_DIR.exists():
        logger.error(f"âŒ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {DOCUMENTS_DIR}")
        return documents

    txt_files = list(DOCUMENTS_DIR.glob("*.txt"))
    if not txt_files:
        logger.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª .txt ÙÙŠ Ù…Ø¬Ù„Ø¯ documents/")
        return documents

    for filepath in txt_files:
        try:
            content = filepath.read_text(encoding="utf-8")
            if content.strip():
                doc = Document(
                    page_content=content,
                    metadata={
                        "source": filepath.name,
                        "file_path": str(filepath),
                    }
                )
                documents.append(doc)
                logger.info(f"ğŸ“„ ØªÙ… ØªØ­Ù…ÙŠÙ„: {filepath.name} ({len(content)} Ø­Ø±Ù)")
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© {filepath.name}: {e}")

    logger.info(f"ğŸ“š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…ÙØ­Ù…Ù‘Ù„Ø©: {len(documents)}")
    return documents


def chunk_documents(documents: list[Document]) -> list[Document]:
    """ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹"""
    settings = get_settings()

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=settings.chunk_size,
        chunk_overlap=settings.chunk_overlap,
        separators=["\n\n", "\n", ".", "ØŒ", "ØŸ", "!", " "],
        length_function=len,
    )

    chunks = splitter.split_documents(documents)
    logger.info(f"âœ‚ï¸ ØªÙ… Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ Ø¥Ù„Ù‰ {len(chunks)} Ù…Ù‚Ø·Ø¹")
    return chunks


def store_in_chromadb(chunks: list[Document]):
    """ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ ÙÙŠ ChromaDB"""
    settings = get_settings()

    embeddings = OpenAIEmbeddings(
        model=settings.embedding_model,
        openai_api_key=settings.openai_api_key,
    )

    # Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙŠØ¯Ø©
    persist_dir = settings.chroma_persist_dir
    os.makedirs(persist_dir, exist_ok=True)

    logger.info("ğŸ§  Ø¬Ø§Ø±Ù Ø¥Ù†Ø´Ø§Ø¡ Embeddings ÙˆØªØ®Ø²ÙŠÙ†Ù‡Ø§...")

    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        collection_name=settings.chroma_collection,
        persist_directory=persist_dir,
    )

    count = vectorstore._collection.count()
    logger.info(f"âœ… ØªÙ… ØªØ®Ø²ÙŠÙ† {count} Ù…Ù‚Ø·Ø¹ ÙÙŠ ChromaDB Ø¨Ù†Ø¬Ø§Ø­!")
    return vectorstore


def main():
    """ØªØ´ØºÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¬Ù‡ÙŠØ² Ø§Ù„ÙƒØ§Ù…Ù„Ø©"""
    logger.info("=" * 60)
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ¬Ù‡ÙŠØ² Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©")
    logger.info("=" * 60)

    # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
    documents = load_text_files()
    if not documents:
        logger.error("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§. Ø¶Ø¹ Ù…Ù„ÙØ§Øª .txt ÙÙŠ Ù…Ø¬Ù„Ø¯ documents/")
        sys.exit(1)

    # 2. ØªÙ‚Ø·ÙŠØ¹
    chunks = chunk_documents(documents)

    # 3. ØªØ®Ø²ÙŠÙ†
    store_in_chromadb(chunks)

    logger.info("=" * 60)
    logger.info("ğŸ‰ ØªÙ… ØªØ¬Ù‡ÙŠØ² Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¨Ù†Ø¬Ø§Ø­!")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
